---
title: "Why can L₁ regularization perform feature selection?"
author: "Derek Allums"
categories: [regularization, data-science, machine-learning, lasso, l1, ridge, l2]
date: "2025-10-11"
format:
  html:
    code-fold: false
    toc: true
    toc-depth: 3
    theme: cosmo
jupyter: python3
---

## Introduction

If you've ever trained a regression model, you've probably heard: "Use L₁ for feature selection, L₂ for handling multicollinearity." But *why* does L₁ regularization zero out coefficients while L₂ just makes them smaller? The answer is geometry.

## The Setup: What Are We Optimizing?

In regularized regression, we're trying to minimize a loss function plus a penalty term:

$$
\text{minimize: } \mathcal{L}(\mathbf{w}) + \lambda \cdot \text{Penalty}(\mathbf{w})
$$

Where:
- $\mathcal{L}(\mathbf{w})$ is our loss function (e.g., mean squared error)
- $\lambda$ controls the regularization strength
- The penalty differs between L₁ and L₂

**L₁ (Lasso) penalty:**
$$
\text{Penalty}_{\text{L1}} = \sum_{i=1}^{n} |w_i|
$$

**L₂ (Ridge) penalty:**
$$
\text{Penalty}_{\text{L2}} = \sum_{i=1}^{n} w_i^2
$$

The key question: why does one drive coefficients to exactly zero while the other doesn't?

Note there is good discussion of this all over the interent (https://stats.stackexchange.com/questions/96046/why-l1-norm-can-result-in-variable-selection-but-not-l2)
and notably in Elements of Statistical Learning as well (which that posts references). 

## The Geometric Intuition

Let's visualize this in 2D (two coefficients: $w_1$ and $w_2$). We can reframe the optimization as a constrained problem:

**Minimize the loss subject to:**
$$ L_1: |w_1| + |w_2| \leq t $$
$$ L_2: w_1^2 + w_2^2 \leq t $$

Running some boilerplate code below gives us the diagram that follows

```{python}
import numpy as np
import matplotlib.pyplot as plt

# Create figure with two subplots
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))

# L1 constraint region (diamond)
t = 1.0
w1_l1 = np.array([t, 0, -t, 0, t])
w2_l1 = np.array([0, t, 0, -t, 0])

ax1.fill(w1_l1, w2_l1, alpha=0.3, color='blue', label='Feasible region')
ax1.plot(w1_l1, w2_l1, 'b-', linewidth=2)
ax1.axhline(y=0, color='k', linestyle='-', alpha=0.3)
ax1.axvline(x=0, color='k', linestyle='-', alpha=0.3)
ax1.set_xlabel('w₁', fontsize=12)
ax1.set_ylabel('w₂', fontsize=12)
ax1.set_title('L₁ Constraint: |w₁| + |w₂| ≤ 1', fontsize=14)
ax1.set_xlim(-1.5, 1.5)
ax1.set_ylim(-1.5, 1.5)
ax1.grid(alpha=0.3)
ax1.legend()

# L2 constraint region (circle)
theta = np.linspace(0, 2*np.pi, 100)
w1_l2 = t * np.cos(theta)
w2_l2 = t * np.sin(theta)

ax2.fill(w1_l2, w2_l2, alpha=0.3, color='red', label='Feasible region')
ax2.plot(w1_l2, w2_l2, 'r-', linewidth=2)
ax2.axhline(y=0, color='k', linestyle='-', alpha=0.3)
ax2.axvline(x=0, color='k', linestyle='-', alpha=0.3)
ax2.set_xlabel('w₁', fontsize=12)
ax2.set_ylabel('w₂', fontsize=12)
ax2.set_title('L₂ Constraint: w₁² + w₂² ≤ 1', fontsize=14)
ax2.set_xlim(-1.5, 1.5)
ax2.set_ylim(-1.5, 1.5)
ax2.grid(alpha=0.3)
ax2.legend()

plt.tight_layout()
plt.show()
```

Nothing surprising here, just a buch of code to tell us that the formula already does: one is a square, one is a circle. Why is that important?

## Where Solutions Live

The key realization is: the optimal solution occurs where the loss function contours (ellipses centered at the unconstrained minimum) first touch the constraint region.

Let's plot to see what that looks like (I tried but the diagram linked above is better, so just reference that)

**The key insight:**

- **$L_1$**: The diamond has corners that lie exactly on the axes. When the loss contours expand outward and touch the constraint region, they're very likely to hit a corner first — where one or more coefficients are exactly zero.

- **$L_2$**: The circle has no corners. The loss contours will touch it at some smooth point where all coefficients are small but non-zero.


## The Gradient Perspective

There's another way to understand this: look at the gradients.

**L₁ gradient:**
$$
\frac{\partial}{\partial w_i} |w_i| = \begin{cases}
+1 & \text{if } w_i > 0 \\
-1 & \text{if } w_i < 0 \\
\text{undefined} & \text{if } w_i = 0
\end{cases}
$$

The gradient is constant regardless of how small $w_i$ gets! This creates a "constant push" toward zero.

**L₂ gradient:**
$$
\frac{\partial}{\partial w_i} w_i^2 = 2w_i
$$

As $w_i$ approaches zero, the gradient also approaches zero. The "push" weakens, so coefficients get very small but never quite reach zero.

```{python}
# Visualize the penalty and gradient behavior
w = np.linspace(-2, 2, 1000)
l1_penalty = np.abs(w)
l2_penalty = w**2

# Gradients (subgradient for L1 at 0)
l1_grad = np.sign(w)
l1_grad[w == 0] = 0  # Convention for visualization
l2_grad = 2 * w

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# Penalty functions
ax1.plot(w, l1_penalty, 'b-', linewidth=2, label='L₁: |w|')
ax1.plot(w, l2_penalty, 'r-', linewidth=2, label='L₂: w²')
ax1.axhline(y=0, color='k', linestyle='-', alpha=0.3)
ax1.axvline(x=0, color='k', linestyle='--', alpha=0.3)
ax1.set_xlabel('w', fontsize=12)
ax1.set_ylabel('Penalty', fontsize=12)
ax1.set_title('Penalty Functions', fontsize=14)
ax1.legend(fontsize=11)
ax1.grid(alpha=0.3)
ax1.set_ylim(-0.1, 2)

# Gradients
ax2.plot(w, l1_grad, 'b-', linewidth=2, label='∂|w|/∂w (constant)')
ax2.plot(w, l2_grad, 'r-', linewidth=2, label='∂w²/∂w = 2w (linear)')
ax2.axhline(y=0, color='k', linestyle='-', alpha=0.3)
ax2.axvline(x=0, color='k', linestyle='--', alpha=0.3)
ax2.set_xlabel('w', fontsize=12)
ax2.set_ylabel('Gradient', fontsize=12)
ax2.set_title('Gradient Behavior', fontsize=14)
ax2.legend(fontsize=11)
ax2.grid(alpha=0.3)
ax2.set_ylim(-2.5, 2.5)

plt.tight_layout()
plt.show()
```

Notice: L₁'s gradient stays at ±1 no matter how small the weight is, maintaining constant pressure to reach exactly zero. L₂'s gradient weakens as weights shrink, never quite finishing the job.