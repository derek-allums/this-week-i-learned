[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "posts/temperature-llms/index.html",
    "href": "posts/temperature-llms/index.html",
    "title": "How Temperature Actually Works in LLMs",
    "section": "",
    "text": "Every time you use an LLM, there’s a parameter called “temperature” hiding in the background, fundamentally shaping how creative or conservative the responses are. But what’s actually happening under the hood?\nToday we’re diving deep into the mathematics and implementation of temperature in language models."
  },
  {
    "objectID": "posts/temperature-llms/index.html#introduction",
    "href": "posts/temperature-llms/index.html#introduction",
    "title": "How Temperature Actually Works in LLMs",
    "section": "",
    "text": "Every time you use an LLM, there’s a parameter called “temperature” hiding in the background, fundamentally shaping how creative or conservative the responses are. But what’s actually happening under the hood?\nToday we’re diving deep into the mathematics and implementation of temperature in language models."
  },
  {
    "objectID": "posts/temperature-llms/index.html#the-core-problem-probability-distributions",
    "href": "posts/temperature-llms/index.html#the-core-problem-probability-distributions",
    "title": "How Temperature Actually Works in LLMs",
    "section": "The Core Problem: Probability Distributions",
    "text": "The Core Problem: Probability Distributions\nAt its heart, a language model outputs a probability distribution over its vocabulary for the next token. Let’s say we have a simple vocabulary of 4 words:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Raw logits from the model\nlogits = np.array([2.0, 1.0, 0.5, 0.1])\nvocab = ['the', 'a', 'an', 'some']\n\n# Convert to probabilities with softmax\ndef softmax(x):\n    exp_x = np.exp(x - np.max(x))  # Subtract max for numerical stability\n    return exp_x / exp_x.sum()\n\nprobs = softmax(logits)\n\n# Visualize\nplt.figure(figsize=(10, 5))\nplt.bar(vocab, probs, color='steelblue')\nplt.ylabel('Probability')\nplt.title('Token Probabilities (Temperature = 1.0)')\nplt.ylim(0, 1)\nfor i, (word, prob) in enumerate(zip(vocab, probs)):\n    plt.text(i, prob + 0.02, f'{prob:.3f}', ha='center')\nplt.show()\n\n\n\n\n\n\n\n\nThe model might assign probabilities like: the: 0.52, a: 0.29, an: 0.13, some: 0.06"
  },
  {
    "objectID": "posts/temperature-llms/index.html#the-mathematics-of-temperature",
    "href": "posts/temperature-llms/index.html#the-mathematics-of-temperature",
    "title": "How Temperature Actually Works in LLMs",
    "section": "The Mathematics of Temperature",
    "text": "The Mathematics of Temperature\nTemperature \\(T\\) is applied before the softmax function. The modified softmax becomes:\n\\[\nP(x_i) = \\frac{e^{z_i/T}}{\\sum_{j=1}^{n} e^{z_j/T}}\n\\]\nWhere: - \\(z_i\\) are the raw logits from the model - \\(T\\) is the temperature parameter - \\(P(x_i)\\) is the probability of token \\(i\\)\n\nWhat Different Temperatures Do\n\ndef softmax_with_temperature(logits, temperature):\n    scaled_logits = logits / temperature\n    return softmax(scaled_logits)\n\ntemperatures = [0.1, 0.5, 1.0, 2.0, 5.0]\n\nfig, axes = plt.subplots(2, 3, figsize=(15, 8))\naxes = axes.flatten()\n\nfor idx, temp in enumerate(temperatures):\n    probs_temp = softmax_with_temperature(logits, temp)\n    \n    axes[idx].bar(vocab, probs_temp, color='steelblue')\n    axes[idx].set_ylabel('Probability')\n    axes[idx].set_title(f'Temperature = {temp}')\n    axes[idx].set_ylim(0, 1)\n    \n    for i, (word, prob) in enumerate(zip(vocab, probs_temp)):\n        axes[idx].text(i, prob + 0.02, f'{prob:.3f}', ha='center', fontsize=9)\n\n# Remove extra subplot\nfig.delaxes(axes[5])\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nThe Entropy Perspective\nWe can measure the “randomness” of a distribution using entropy:\n\\[\nH(P) = -\\sum_{i=1}^{n} P(x_i) \\log P(x_i)\n\\]\nHigher temperature → higher entropy → more random outputs\n\ndef entropy(probs):\n    return -np.sum(probs * np.log(probs + 1e-10))\n\ntemp_range = np.linspace(0.1, 5.0, 100)\nentropies = [entropy(softmax_with_temperature(logits, t)) for t in temp_range]\n\nplt.figure(figsize=(10, 5))\nplt.plot(temp_range, entropies, linewidth=2, color='darkred')\nplt.xlabel('Temperature')\nplt.ylabel('Entropy (bits)')\nplt.title('How Temperature Affects Distribution Entropy')\nplt.grid(alpha=0.3)\nplt.show()"
  },
  {
    "objectID": "posts/temperature-llms/index.html#real-implementation",
    "href": "posts/temperature-llms/index.html#real-implementation",
    "title": "How Temperature Actually Works in LLMs",
    "section": "Real Implementation",
    "text": "Real Implementation\nHere’s how this looks in actual PyTorch code:\nimport torch\nimport torch.nn.functional as F\n\ndef sample_with_temperature(logits, temperature=1.0):\n    \"\"\"\n    Sample from a language model with temperature scaling.\n    \n    Args:\n        logits: Raw output scores from model (shape: [vocab_size])\n        temperature: Sampling temperature (default: 1.0)\n    \n    Returns:\n        Sampled token index\n    \"\"\"\n    # Scale logits by temperature\n    scaled_logits = logits / temperature\n    \n    # Convert to probabilities\n    probs = F.softmax(scaled_logits, dim=-1)\n    \n    # Sample from the distribution\n    token_id = torch.multinomial(probs, num_samples=1)\n    \n    return token_id\n\n# Example usage\nlogits = torch.tensor([2.0, 1.0, 0.5, 0.1])\n\n# Sample 1000 times at different temperatures\nfor temp in [0.5, 1.0, 2.0]:\n    samples = [sample_with_temperature(logits, temp).item() \n               for _ in range(1000)]\n    \n    # Count occurrences\n    counts = np.bincount(samples, minlength=4)\n    empirical_probs = counts / 1000\n    \n    print(f\"\\nTemperature {temp}:\")\n    for word, prob in zip(vocab, empirical_probs):\n        print(f\"  {word}: {prob:.3f}\")"
  },
  {
    "objectID": "posts/temperature-llms/index.html#edge-cases",
    "href": "posts/temperature-llms/index.html#edge-cases",
    "title": "How Temperature Actually Works in LLMs",
    "section": "Edge Cases",
    "text": "Edge Cases\nTemperature = 0: Not technically possible (division by zero), but temperature → 0 approximates greedy decoding (always pick the highest probability token).\nTemperature &lt; 1: “Sharpens” the distribution, making high-probability tokens even more likely.\nTemperature = 1: No modification, the original model distribution.\nTemperature &gt; 1: “Flattens” the distribution, giving lower-probability tokens more chance."
  },
  {
    "objectID": "posts/temperature-llms/index.html#practical-guidelines",
    "href": "posts/temperature-llms/index.html#practical-guidelines",
    "title": "How Temperature Actually Works in LLMs",
    "section": "Practical Guidelines",
    "text": "Practical Guidelines\nFrom my experiments with various models:\n\nT = 0.1-0.3: Factual tasks, code generation, when you need consistency\nT = 0.7-0.9: General conversation, slight creativity\nT = 1.0-1.5: Creative writing, brainstorming\nT = 2.0+: Experimental/chaotic, rarely useful"
  },
  {
    "objectID": "posts/temperature-llms/index.html#conclusion",
    "href": "posts/temperature-llms/index.html#conclusion",
    "title": "How Temperature Actually Works in LLMs",
    "section": "Conclusion",
    "text": "Conclusion\nTemperature is deceptively simple - just one division operation - but it gives us fine-grained control over the exploration/exploitation tradeoff in language generation. Understanding the mathematical foundation helps you know exactly when and how to tune it.\nNext week: How Go channels actually work under the hood (spoiler: there’s a mutex and a circular buffer involved).\n\nFound this useful? Let me know what deep dive you’d like to see next."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "this-week-i-learned",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  }
]