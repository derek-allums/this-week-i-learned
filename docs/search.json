[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Derek Allums, I have a PhD in math and have spent most of my career as a data scientist and recently a data engineer. I realized there are a lot of topics I have some passing familiarity with, e.g., “\\(L_1\\) regularization sometimes acts as feature selection”, but for many of these, how it works under the hood isn’t as clear (in this case, it comes back to geometry - see my second post).\nThe goal of this blog is to provide an outlet for these explorations in the hopes that maybe it could be useful for others - but mainly for me, since I find there’s no better way to understand something than to write about it since any gaps in the understanding become very clear.\nNote: none of this is original research, it is just summary to help with my own understanding. I also make frequent use of LLMs to generate exmaple code, figures, etc."
  },
  {
    "objectID": "posts/temperature-llms/index.html",
    "href": "posts/temperature-llms/index.html",
    "title": "How Temperature Actually Works in LLMs",
    "section": "",
    "text": "Every time you use an LLM, there’s a parameter called “temperature” typically set at \\(0.7\\) or even hidden from the user, fundamentally shaping how creative or conservative the responses are.\nBut what’s actually happening under the hood?"
  },
  {
    "objectID": "posts/temperature-llms/index.html#introduction",
    "href": "posts/temperature-llms/index.html#introduction",
    "title": "How Temperature Actually Works in LLMs",
    "section": "",
    "text": "Every time you use an LLM, there’s a parameter called “temperature” typically set at \\(0.7\\) or even hidden from the user, fundamentally shaping how creative or conservative the responses are.\nBut what’s actually happening under the hood?"
  },
  {
    "objectID": "posts/temperature-llms/index.html#the-core-problem-probability-distributions",
    "href": "posts/temperature-llms/index.html#the-core-problem-probability-distributions",
    "title": "How Temperature Actually Works in LLMs",
    "section": "The Core Problem: Probability Distributions",
    "text": "The Core Problem: Probability Distributions\nAt its core, a language model outputs a probability distribution over its vocabulary for the next token.\nHere’s a toy example on a simple vocabulary of 4 words:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Raw logits from the model\nlogits = np.array([2.0, 1.0, 0.5, 0.1])\nvocab = ['the', 'a', 'an', 'some']\n\n# Convert to probabilities with softmax\ndef softmax(x):\n    exp_x = np.exp(x - np.max(x))  # Subtract max for numerical stability\n    return exp_x / exp_x.sum()\n\nprobs = softmax(logits)\n\n# Visualize\nplt.figure(figsize=(10, 5))\nplt.bar(vocab, probs, color='steelblue')\nplt.ylabel('Probability')\nplt.title('Token Probabilities (Temperature = 1.0)')\nplt.ylim(0, 1)\nfor i, (word, prob) in enumerate(zip(vocab, probs)):\n    plt.text(i, prob + 0.02, f'{prob:.3f}', ha='center')\nplt.show()\n\n\n\n\n\n\n\n\nThe model might assign probabilities like: the: 0.52, a: 0.29, an: 0.13, some: 0.06"
  },
  {
    "objectID": "posts/temperature-llms/index.html#the-mathematics-of-temperature",
    "href": "posts/temperature-llms/index.html#the-mathematics-of-temperature",
    "title": "How Temperature Actually Works in LLMs",
    "section": "The Mathematics of Temperature",
    "text": "The Mathematics of Temperature\nTemperature \\(T\\) is applied before the softmax function. The modified softmax becomes:\n\\[\nP(x_i) = \\frac{e^{z_i/T}}{\\sum_{j=1}^{n} e^{z_j/T}}\n\\]\nWhere: - \\(z_i\\) are the raw logits from the model - \\(T\\) is the temperature parameter - \\(P(x_i)\\) is the probability of token \\(i\\)\nFor those famiilar with, e.g., OpenAI’s API, you’re immediately wondering “wait I thought it was between 0 and 2? This could be between \\(\\epsilon &gt; 0\\) and \\(\\infty\\).” True - they basically normalize these values.\n\nWhat Different Temperatures Do\nOne way to think about this is it’s basically a dampening effect. When we say “creativity” we mean (I guess) “randomness”. Conversely, “deterministic” is just that.\n\ndef softmax_with_temperature(logits, temperature):\n    scaled_logits = logits / temperature\n    return softmax(scaled_logits)\n\ntemperatures = [0.1, 0.5, 1.0, 2.0, 5.0]\n\nfig, axes = plt.subplots(2, 3, figsize=(15, 8))\naxes = axes.flatten()\n\nfor idx, temp in enumerate(temperatures):\n    probs_temp = softmax_with_temperature(logits, temp)\n    \n    axes[idx].bar(vocab, probs_temp, color='steelblue')\n    axes[idx].set_ylabel('Probability')\n    axes[idx].set_title(f'Temperature = {temp}')\n    axes[idx].set_ylim(0, 1)\n    \n    for i, (word, prob) in enumerate(zip(vocab, probs_temp)):\n        axes[idx].text(i, prob + 0.02, f'{prob:.3f}', ha='center', fontsize=9)\n\n# Remove extra subplot\nfig.delaxes(axes[5])\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nThe Entropy Perspective\nContinuing the comments above, we can sort of formalize this with entropy, thinking of it as a measure of the “randomness” of a distribution:\n\\[\nH(P) = -\\sum_{i=1}^{n} P(x_i) \\log P(x_i)\n\\]\nHigher temperature → higher entropy → more random outputs\n\ndef entropy(probs):\n    return -np.sum(probs * np.log(probs + 1e-10))\n\ntemp_range = np.linspace(0.1, 5.0, 100)\nentropies = [entropy(softmax_with_temperature(logits, t)) for t in temp_range]\n\nplt.figure(figsize=(10, 5))\nplt.plot(temp_range, entropies, linewidth=2, color='darkred')\nplt.xlabel('Temperature')\nplt.ylabel('Entropy (bits)')\nplt.title('How Temperature Affects Distribution Entropy')\nplt.grid(alpha=0.3)\nplt.show()"
  },
  {
    "objectID": "posts/temperature-llms/index.html#real-implementation",
    "href": "posts/temperature-llms/index.html#real-implementation",
    "title": "How Temperature Actually Works in LLMs",
    "section": "Real Implementation",
    "text": "Real Implementation\nHere’s how this looks in actual PyTorch code:\nimport torch\nimport torch.nn.functional as F\n\ndef sample_with_temperature(logits, temperature=1.0):\n    \"\"\"\n    Sample from a language model with temperature scaling.\n    \n    Args:\n        logits: Raw output scores from model (shape: [vocab_size])\n        temperature: Sampling temperature (default: 1.0)\n    \n    Returns:\n        Sampled token index\n    \"\"\"\n    # Scale logits by temperature\n    scaled_logits = logits / temperature\n    \n    # Convert to probabilities\n    probs = F.softmax(scaled_logits, dim=-1)\n    \n    # Sample from the distribution\n    token_id = torch.multinomial(probs, num_samples=1)\n    \n    return token_id\n\n# Example usage\nlogits = torch.tensor([2.0, 1.0, 0.5, 0.1])\n\n# Sample 1000 times at different temperatures\nfor temp in [0.5, 1.0, 2.0]:\n    samples = [sample_with_temperature(logits, temp).item() \n               for _ in range(1000)]\n    \n    # Count occurrences\n    counts = np.bincount(samples, minlength=4)\n    empirical_probs = counts / 1000\n    \n    print(f\"\\nTemperature {temp}:\")\n    for word, prob in zip(vocab, empirical_probs):\n        print(f\"  {word}: {prob:.3f}\")"
  },
  {
    "objectID": "posts/temperature-llms/index.html#heuristics",
    "href": "posts/temperature-llms/index.html#heuristics",
    "title": "How Temperature Actually Works in LLMs",
    "section": "Heuristics",
    "text": "Heuristics\nTemperature = 0: Not technically possible (division by zero), but temperature → 0 approximates greedy decoding (always pick the highest probability token). Thinking back to the definition and examples above, you basically exaggerate the differences in probabilities between the next tokens, so that eventually it’s deterministic or very close to it.\nTemperature &lt; 1: “Sharpens” the distribution, making high-probability tokens even more likely.\nTemperature = 1: No modification, the original model distribution.\nTemperature &gt; 1: “Flattens” the distribution, giving lower-probability tokens more chance."
  },
  {
    "objectID": "posts/temperature-llms/index.html#practical-guidelines",
    "href": "posts/temperature-llms/index.html#practical-guidelines",
    "title": "How Temperature Actually Works in LLMs",
    "section": "Practical Guidelines",
    "text": "Practical Guidelines\nHere are some guidelines that you can find all over the internet (referencing the OpenAI normalized values). Better heuristics are available as well.\n\nT = 0.1-0.3: Factual tasks, code generation, when you need consistency\nT = 0.7-0.9: General conversation, slight creativity\nT = 1.0-1.5: Creative writing, brainstorming\nT = 2.0+: Experimental/chaotic, rarely useful"
  },
  {
    "objectID": "posts/l1-versus-l2/index.html",
    "href": "posts/l1-versus-l2/index.html",
    "title": "Why can L₁ regularization perform feature selection?",
    "section": "",
    "text": "If you’ve ever trained a regression model, you’ve probably heard: “Use L₁ for feature selection, L₂ for handling multicollinearity.” But why does L₁ regularization zero out coefficients while L₂ just makes them smaller? The answer is geometry."
  },
  {
    "objectID": "posts/l1-versus-l2/index.html#introduction",
    "href": "posts/l1-versus-l2/index.html#introduction",
    "title": "Why can L₁ regularization perform feature selection?",
    "section": "",
    "text": "If you’ve ever trained a regression model, you’ve probably heard: “Use L₁ for feature selection, L₂ for handling multicollinearity.” But why does L₁ regularization zero out coefficients while L₂ just makes them smaller? The answer is geometry."
  },
  {
    "objectID": "posts/l1-versus-l2/index.html#the-setup-what-are-we-optimizing",
    "href": "posts/l1-versus-l2/index.html#the-setup-what-are-we-optimizing",
    "title": "Why can L₁ regularization perform feature selection?",
    "section": "The Setup: What Are We Optimizing?",
    "text": "The Setup: What Are We Optimizing?\nIn regularized regression, we’re trying to minimize a loss function plus a penalty term:\n\\[\n\\text{minimize: } \\mathcal{L}(\\mathbf{w}) + \\lambda \\cdot \\text{Penalty}(\\mathbf{w})\n\\]\nWhere: - \\(\\mathcal{L}(\\mathbf{w})\\) is our loss function (e.g., mean squared error) - \\(\\lambda\\) controls the regularization strength - The penalty differs between L₁ and L₂\nL₁ (Lasso) penalty: \\[\n\\text{Penalty}_{\\text{L1}} = \\sum_{i=1}^{n} |w_i|\n\\]\nL₂ (Ridge) penalty: \\[\n\\text{Penalty}_{\\text{L2}} = \\sum_{i=1}^{n} w_i^2\n\\]\nThe key question: why does one drive coefficients to exactly zero while the other doesn’t?\nNote there is good discussion of this all over the interent (https://stats.stackexchange.com/questions/96046/why-l1-norm-can-result-in-variable-selection-but-not-l2) and notably in Elements of Statistical Learning as well (which that posts references)."
  },
  {
    "objectID": "posts/l1-versus-l2/index.html#the-geometric-intuition",
    "href": "posts/l1-versus-l2/index.html#the-geometric-intuition",
    "title": "Why can L₁ regularization perform feature selection?",
    "section": "The Geometric Intuition",
    "text": "The Geometric Intuition\nLet’s visualize this in 2D (two coefficients: \\(w_1\\) and \\(w_2\\)). We can reframe the optimization as a constrained problem:\nMinimize the loss subject to: \\[ L_1: |w_1| + |w_2| \\leq t \\] \\[ L_2: w_1^2 + w_2^2 \\leq t \\]\nRunning some boilerplate code below gives us the diagram that follows\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Create figure with two subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n\n# L1 constraint region (diamond)\nt = 1.0\nw1_l1 = np.array([t, 0, -t, 0, t])\nw2_l1 = np.array([0, t, 0, -t, 0])\n\nax1.fill(w1_l1, w2_l1, alpha=0.3, color='blue', label='Feasible region')\nax1.plot(w1_l1, w2_l1, 'b-', linewidth=2)\nax1.axhline(y=0, color='k', linestyle='-', alpha=0.3)\nax1.axvline(x=0, color='k', linestyle='-', alpha=0.3)\nax1.set_xlabel('w₁', fontsize=12)\nax1.set_ylabel('w₂', fontsize=12)\nax1.set_title('L₁ Constraint: |w₁| + |w₂| ≤ 1', fontsize=14)\nax1.set_xlim(-1.5, 1.5)\nax1.set_ylim(-1.5, 1.5)\nax1.grid(alpha=0.3)\nax1.legend()\n\n# L2 constraint region (circle)\ntheta = np.linspace(0, 2*np.pi, 100)\nw1_l2 = t * np.cos(theta)\nw2_l2 = t * np.sin(theta)\n\nax2.fill(w1_l2, w2_l2, alpha=0.3, color='red', label='Feasible region')\nax2.plot(w1_l2, w2_l2, 'r-', linewidth=2)\nax2.axhline(y=0, color='k', linestyle='-', alpha=0.3)\nax2.axvline(x=0, color='k', linestyle='-', alpha=0.3)\nax2.set_xlabel('w₁', fontsize=12)\nax2.set_ylabel('w₂', fontsize=12)\nax2.set_title('L₂ Constraint: w₁² + w₂² ≤ 1', fontsize=14)\nax2.set_xlim(-1.5, 1.5)\nax2.set_ylim(-1.5, 1.5)\nax2.grid(alpha=0.3)\nax2.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nNothing surprising here, just a buch of code to tell us that the formula already does: one is a square, one is a circle. Why is that important?"
  },
  {
    "objectID": "posts/l1-versus-l2/index.html#where-solutions-live",
    "href": "posts/l1-versus-l2/index.html#where-solutions-live",
    "title": "Why can L₁ regularization perform feature selection?",
    "section": "Where Solutions Live",
    "text": "Where Solutions Live\nThe key realization is: the optimal solution occurs where the loss function contours (ellipses centered at the unconstrained minimum) first touch the constraint region.\nLet’s plot to see what that looks like (I tried but the diagram linked above is better, so just reference that)\nThe key insight:\n\n\\(L_1\\): The diamond has corners that lie exactly on the axes. When the loss contours expand outward and touch the constraint region, they’re very likely to hit a corner first — where one or more coefficients are exactly zero.\n\\(L_2\\): The circle has no corners. The loss contours will touch it at some smooth point where all coefficients are small but non-zero."
  },
  {
    "objectID": "posts/l1-versus-l2/index.html#the-gradient-perspective",
    "href": "posts/l1-versus-l2/index.html#the-gradient-perspective",
    "title": "Why can L₁ regularization perform feature selection?",
    "section": "The Gradient Perspective",
    "text": "The Gradient Perspective\nThere’s another way to understand this: look at the gradients.\nL₁ gradient: \\[\n\\frac{\\partial}{\\partial w_i} |w_i| = \\begin{cases}\n+1 & \\text{if } w_i &gt; 0 \\\\\n-1 & \\text{if } w_i &lt; 0 \\\\\n\\text{undefined} & \\text{if } w_i = 0\n\\end{cases}\n\\]\nThe gradient is constant regardless of how small \\(w_i\\) gets! This creates a “constant push” toward zero.\nL₂ gradient: \\[\n\\frac{\\partial}{\\partial w_i} w_i^2 = 2w_i\n\\]\nAs \\(w_i\\) approaches zero, the gradient also approaches zero. The “push” weakens, so coefficients get very small but never quite reach zero.\n\n# Visualize the penalty and gradient behavior\nw = np.linspace(-2, 2, 1000)\nl1_penalty = np.abs(w)\nl2_penalty = w**2\n\n# Gradients (subgradient for L1 at 0)\nl1_grad = np.sign(w)\nl1_grad[w == 0] = 0  # Convention for visualization\nl2_grad = 2 * w\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n# Penalty functions\nax1.plot(w, l1_penalty, 'b-', linewidth=2, label='L₁: |w|')\nax1.plot(w, l2_penalty, 'r-', linewidth=2, label='L₂: w²')\nax1.axhline(y=0, color='k', linestyle='-', alpha=0.3)\nax1.axvline(x=0, color='k', linestyle='--', alpha=0.3)\nax1.set_xlabel('w', fontsize=12)\nax1.set_ylabel('Penalty', fontsize=12)\nax1.set_title('Penalty Functions', fontsize=14)\nax1.legend(fontsize=11)\nax1.grid(alpha=0.3)\nax1.set_ylim(-0.1, 2)\n\n# Gradients\nax2.plot(w, l1_grad, 'b-', linewidth=2, label='∂|w|/∂w (constant)')\nax2.plot(w, l2_grad, 'r-', linewidth=2, label='∂w²/∂w = 2w (linear)')\nax2.axhline(y=0, color='k', linestyle='-', alpha=0.3)\nax2.axvline(x=0, color='k', linestyle='--', alpha=0.3)\nax2.set_xlabel('w', fontsize=12)\nax2.set_ylabel('Gradient', fontsize=12)\nax2.set_title('Gradient Behavior', fontsize=14)\nax2.legend(fontsize=11)\nax2.grid(alpha=0.3)\nax2.set_ylim(-2.5, 2.5)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nNotice: L₁’s gradient stays at ±1 no matter how small the weight is, maintaining constant pressure to reach exactly zero. L₂’s gradient weakens as weights shrink, never quite finishing the job."
  },
  {
    "objectID": "posts/searching-in-vector-dbs/index.html",
    "href": "posts/searching-in-vector-dbs/index.html",
    "title": "Searching in vector DBs: BM25 and semantic search",
    "section": "",
    "text": "Vector databases are everywhere now - storing embeddings and doing semantic search with cosine similarity. But here’s something interesting: most production vector databases also implement BM25, a 40-year-old keyword search algorithm. Why?\nBecause pure semantic search has blind spots that BM25 covers beautifully. Today we’re diving into how BM25 actually works, why it complements embeddings, and how hybrid search combines both."
  },
  {
    "objectID": "posts/searching-in-vector-dbs/index.html#introduction",
    "href": "posts/searching-in-vector-dbs/index.html#introduction",
    "title": "Searching in vector DBs: BM25 and semantic search",
    "section": "",
    "text": "Vector databases are everywhere now - storing embeddings and doing semantic search with cosine similarity. But here’s something interesting: most production vector databases also implement BM25, a 40-year-old keyword search algorithm. Why?\nBecause pure semantic search has blind spots that BM25 covers beautifully. Today we’re diving into how BM25 actually works, why it complements embeddings, and how hybrid search combines both."
  },
  {
    "objectID": "posts/searching-in-vector-dbs/index.html#the-problem-with-pure-semantic-search",
    "href": "posts/searching-in-vector-dbs/index.html#the-problem-with-pure-semantic-search",
    "title": "Searching in vector DBs: BM25 and semantic search",
    "section": "The Problem with Pure Semantic Search",
    "text": "The Problem with Pure Semantic Search\nImagine you’re searching for documents about “GPT-4” in a database. With pure embedding search:\n\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Simplified: query and document embeddings\nquery_embedding = np.random.randn(1, 768)  # \"GPT-4\"\ndoc1_embedding = np.random.randn(1, 768)   # Contains \"GPT-4\"\ndoc2_embedding = np.random.randn(1, 768)   # About \"large language models\"\n\n# Semantic similarity\nsim1 = cosine_similarity(query_embedding, doc1_embedding)[0][0]\nsim2 = cosine_similarity(query_embedding, doc2_embedding)[0][0]\n\nprint(f\"Document mentioning 'GPT-4': {sim1:.3f}\")\nprint(f\"Document about LLMs generally: {sim2:.3f}\")\n\nDocument mentioning 'GPT-4': -0.002\nDocument about LLMs generally: -0.033\n\n\nThe issue? Embeddings might rate a document about “large language models in general” as more semantically similar than a document that explicitly mentions “GPT-4”, even though you wanted exact matches.\nSemantic search fails at: - Exact keyword matches (product codes, names, technical terms) - Rare or new terms the embedding model wasn’t trained on - Cases where word frequency matters (“mentioned 10 times” vs “mentioned once”)\nThis is where BM25 shines."
  },
  {
    "objectID": "posts/searching-in-vector-dbs/index.html#what-is-bm25",
    "href": "posts/searching-in-vector-dbs/index.html#what-is-bm25",
    "title": "Searching in vector DBs: BM25 and semantic search",
    "section": "What is BM25?",
    "text": "What is BM25?\nBM25 (Best Matching 25) is a ranking function from the 1970s-90s that scores documents based on term frequency and document length. It’s probabilistic and has solid theoretical foundations.\nThe formula looks intimidating but breaks down nicely:\n\\[\n\\text{BM25}(D, Q) = \\sum_{i=1}^{n} \\text{IDF}(q_i) \\cdot \\frac{f(q_i, D) \\cdot (k_1 + 1)}{f(q_i, D) + k_1 \\cdot \\left(1 - b + b \\cdot \\frac{|D|}{\\text{avgdl}}\\right)}\n\\]\nWhere: - \\(Q\\) = query terms \\(\\{q_1, q_2, ..., q_n\\}\\) - \\(D\\) = document - \\(f(q_i, D)\\) = frequency of term \\(q_i\\) in document \\(D\\) - \\(|D|\\) = length of document \\(D\\) - \\(\\text{avgdl}\\) = average document length in the collection - \\(k_1\\) and \\(b\\) = tuning parameters (typically \\(k_1=1.5\\), \\(b=0.75\\))\nLet’s break down each component."
  },
  {
    "objectID": "posts/searching-in-vector-dbs/index.html#component-1-term-frequency-tf",
    "href": "posts/searching-in-vector-dbs/index.html#component-1-term-frequency-tf",
    "title": "Searching in vector DBs: BM25 and semantic search",
    "section": "Component 1: Term Frequency (TF)",
    "text": "Component 1: Term Frequency (TF)\nThe first part captures “how often does this query term appear?”\n\\[\n\\text{TF}(q_i, D) = \\frac{f(q_i, D) \\cdot (k_1 + 1)}{f(q_i, D) + k_1 \\cdot \\left(1 - b + b \\cdot \\frac{|D|}{\\text{avgdl}}\\right)}\n\\]\nNotice this is saturating - if a term appears 100 times, it doesn’t get 100x the score. It has diminishing returns.\n\nimport matplotlib.pyplot as plt\n\ndef bm25_tf(term_freq, doc_length, avg_doc_length, k1=1.5, b=0.75):\n    \"\"\"Calculate BM25 term frequency component\"\"\"\n    numerator = term_freq * (k1 + 1)\n    denominator = term_freq + k1 * (1 - b + b * (doc_length / avg_doc_length))\n    return numerator / denominator\n\n# Visualize how TF score saturates\nterm_frequencies = np.arange(1, 50)\navg_length = 1000\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n# Effect of term frequency\nfor doc_len in [500, 1000, 2000]:\n    scores = [bm25_tf(tf, doc_len, avg_length) for tf in term_frequencies]\n    ax1.plot(term_frequencies, scores, label=f'Doc length: {doc_len}', linewidth=2)\n\nax1.set_xlabel('Term Frequency', fontsize=12)\nax1.set_ylabel('BM25 TF Score', fontsize=12)\nax1.set_title('BM25 Term Frequency: Saturating Behavior', fontsize=14)\nax1.legend()\nax1.grid(alpha=0.3)\n\n# Compare to linear TF\nlinear_tf = term_frequencies / max(term_frequencies)\nbm25_tf_scores = [bm25_tf(tf, 1000, 1000) for tf in term_frequencies]\n\nax2.plot(term_frequencies, linear_tf, 'r--', label='Linear TF', linewidth=2)\nax2.plot(term_frequencies, bm25_tf_scores, 'b-', label='BM25 TF', linewidth=2)\nax2.set_xlabel('Term Frequency', fontsize=12)\nax2.set_ylabel('Normalized Score', fontsize=12)\nax2.set_title('BM25 vs Linear Term Frequency', fontsize=14)\nax2.legend()\nax2.grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nKey insight: After ~10 occurrences, additional mentions barely increase the score. This prevents keyword stuffing from gaming the system."
  },
  {
    "objectID": "posts/searching-in-vector-dbs/index.html#component-2-inverse-document-frequency-idf",
    "href": "posts/searching-in-vector-dbs/index.html#component-2-inverse-document-frequency-idf",
    "title": "Searching in vector DBs: BM25 and semantic search",
    "section": "Component 2: Inverse Document Frequency (IDF)",
    "text": "Component 2: Inverse Document Frequency (IDF)\nIDF measures “how rare is this term across all documents?”\n\\[\n\\text{IDF}(q_i) = \\ln\\left(\\frac{N - n(q_i) + 0.5}{n(q_i) + 0.5} + 1\\right)\n\\]\nWhere: - \\(N\\) = total number of documents - \\(n(q_i)\\) = number of documents containing term \\(q_i\\)\nCommon words like “the” or “is” appear in most documents → low IDF → low contribution to score. Rare terms like “BM25” or “GPT-4” → high IDF → high contribution.\n\ndef bm25_idf(num_docs_with_term, total_docs):\n    \"\"\"Calculate BM25 IDF score\"\"\"\n    numerator = total_docs - num_docs_with_term + 0.5\n    denominator = num_docs_with_term + 0.5\n    return np.log((numerator / denominator) + 1)\n\n# Visualize IDF scores\ntotal_docs = 10000\ndoc_frequencies = np.logspace(0, 4, 100, dtype=int)  # 1 to 10000\ndoc_frequencies = np.clip(doc_frequencies, 1, total_docs)\n\nidf_scores = [bm25_idf(df, total_docs) for df in doc_frequencies]\n\nplt.figure(figsize=(10, 6))\nplt.semilogx(doc_frequencies, idf_scores, 'b-', linewidth=2)\nplt.xlabel('Number of Documents Containing Term', fontsize=12)\nplt.ylabel('IDF Score', fontsize=12)\nplt.title('BM25 IDF: Rare Terms Get Higher Scores', fontsize=14)\nplt.grid(alpha=0.3, which='both')\n\n# Mark some example points\nexamples = [\n    (1, \"Unique term\"),\n    (100, \"Rare term\"),\n    (1000, \"Common term\"),\n    (9000, \"Stop word\")\n]\n\nfor df, label in examples:\n    idf = bm25_idf(df, total_docs)\n    plt.plot(df, idf, 'ro', markersize=10)\n    plt.annotate(label, xy=(df, idf), xytext=(df*1.5, idf+0.3),\n                fontsize=10, arrowprops=dict(arrowstyle='-&gt;', color='red'))\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/searching-in-vector-dbs/index.html#component-3-document-length-normalization",
    "href": "posts/searching-in-vector-dbs/index.html#component-3-document-length-normalization",
    "title": "Searching in vector DBs: BM25 and semantic search",
    "section": "Component 3: Document Length Normalization",
    "text": "Component 3: Document Length Normalization\nThe parameter \\(b\\) (typically 0.75) controls how much we penalize long documents:\n\\[\n\\text{Norm} = 1 - b + b \\cdot \\frac{|D|}{\\text{avgdl}}\n\\]\n\n\\(b = 0\\): No length normalization (long docs favored)\n\\(b = 1\\): Full length normalization (heavily penalize long docs)\n\\(b = 0.75\\): Balanced (standard)\n\nWhy normalize? Long documents naturally contain more term occurrences. Without normalization, they’d dominate results unfairly.\n\n# Effect of b parameter on document length normalization\ndoc_lengths = np.linspace(100, 5000, 100)\navg_doc_length = 1000\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\nfor b in [0.0, 0.5, 0.75, 1.0]:\n    norm_factors = 1 - b + b * (doc_lengths / avg_doc_length)\n    ax.plot(doc_lengths, norm_factors, label=f'b = {b}', linewidth=2)\n\nax.axvline(avg_doc_length, color='gray', linestyle='--', alpha=0.5, label='Avg doc length')\nax.set_xlabel('Document Length', fontsize=12)\nax.set_ylabel('Normalization Factor', fontsize=12)\nax.set_title('Effect of Parameter b on Length Normalization', fontsize=14)\nax.legend()\nax.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/searching-in-vector-dbs/index.html#complete-implementation",
    "href": "posts/searching-in-vector-dbs/index.html#complete-implementation",
    "title": "Searching in vector DBs: BM25 and semantic search",
    "section": "Complete Implementation",
    "text": "Complete Implementation\nLet’s implement BM25 from scratch:\nimport numpy as np\nfrom collections import Counter\nimport re\n\nclass BM25:\n    def __init__(self, k1=1.5, b=0.75):\n        self.k1 = k1\n        self.b = b\n        self.doc_freqs = []\n        self.idf = {}\n        self.doc_len = []\n        self.avgdl = 0\n        self.N = 0\n        \n    def tokenize(self, text):\n        \"\"\"Simple tokenization\"\"\"\n        return re.findall(r'\\w+', text.lower())\n    \n    def fit(self, documents):\n        \"\"\"Build IDF scores from document collection\"\"\"\n        self.N = len(documents)\n        \n        # Calculate document frequencies and lengths\n        df = Counter()\n        for doc in documents:\n            tokens = self.tokenize(doc)\n            self.doc_len.append(len(tokens))\n            unique_tokens = set(tokens)\n            df.update(unique_tokens)\n        \n        self.avgdl = sum(self.doc_len) / self.N\n        \n        # Calculate IDF for each term\n        for term, freq in df.items():\n            self.idf[term] = np.log((self.N - freq + 0.5) / (freq + 0.5) + 1)\n    \n    def score(self, query, document_idx, document):\n        \"\"\"Calculate BM25 score for a query-document pair\"\"\"\n        query_terms = self.tokenize(query)\n        doc_terms = self.tokenize(document)\n        doc_term_freqs = Counter(doc_terms)\n        \n        score = 0.0\n        doc_len = self.doc_len[document_idx]\n        \n        for term in query_terms:\n            if term not in self.idf:\n                continue\n                \n            idf = self.idf[term]\n            tf = doc_term_freqs.get(term, 0)\n            \n            # BM25 formula\n            numerator = tf * (self.k1 + 1)\n            denominator = tf + self.k1 * (1 - self.b + self.b * (doc_len / self.avgdl))\n            \n            score += idf * (numerator / denominator)\n        \n        return score\n\n# Example usage\ndocuments = [\n    \"BM25 is a ranking function used in information retrieval\",\n    \"Vector databases store embeddings for semantic search\",\n    \"BM25 and vector search complement each other in hybrid systems\",\n    \"The quick brown fox jumps over the lazy dog\",\n]\n\nbm25 = BM25()\nbm25.fit(documents)\n\nquery = \"BM25 vector search\"\nscores = [bm25.score(query, idx, doc) for idx, doc in enumerate(documents)]\n\nprint(\"Query:\", query)\nprint(\"\\nDocument scores:\")\nfor idx, (doc, score) in enumerate(sorted(zip(documents, scores), \n                                          key=lambda x: x[1], reverse=True)):\n    print(f\"{score:.3f}: {doc[:60]}...\")"
  },
  {
    "objectID": "posts/searching-in-vector-dbs/index.html#hybrid-search-best-of-both-worlds",
    "href": "posts/searching-in-vector-dbs/index.html#hybrid-search-best-of-both-worlds",
    "title": "Searching in vector DBs: BM25 and semantic search",
    "section": "Hybrid Search: Best of Both Worlds",
    "text": "Hybrid Search: Best of Both Worlds\nModern vector databases combine BM25 and embedding similarity:\n\\[\n\\text{Hybrid Score} = \\alpha \\cdot \\text{BM25}(Q, D) + (1-\\alpha) \\cdot \\text{Cosine}(\\mathbf{q}, \\mathbf{d})\n\\]\nWhere \\(\\alpha\\) controls the keyword/semantic balance (typically 0.5).\n\n# Simulate hybrid search\nnp.random.seed(42)\n\ndef normalize(scores):\n    \"\"\"Min-max normalization to [0,1]\"\"\"\n    min_s, max_s = min(scores), max(scores)\n    if max_s == min_s:\n        return [0.5] * len(scores)\n    return [(s - min_s) / (max_s - min_s) for s in scores]\n\n# Simulate scores for 5 documents\nbm25_scores = [8.5, 2.1, 6.3, 0.5, 7.8]  # Keyword relevance\nsemantic_scores = [0.72, 0.91, 0.68, 0.45, 0.55]  # Embedding similarity\n\n# Normalize both to [0, 1]\nbm25_norm = normalize(bm25_scores)\nsemantic_norm = normalize(semantic_scores)\n\n# Calculate hybrid scores for different alpha values\nalphas = np.linspace(0, 1, 11)\ndoc_names = [f\"Doc {i+1}\" for i in range(5)]\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n\n# Show individual scores\nx = np.arange(len(doc_names))\nwidth = 0.35\n\nax1.bar(x - width/2, bm25_norm, width, label='BM25 (normalized)', alpha=0.8)\nax1.bar(x + width/2, semantic_norm, width, label='Semantic (normalized)', alpha=0.8)\nax1.set_xlabel('Documents', fontsize=12)\nax1.set_ylabel('Normalized Score', fontsize=12)\nax1.set_title('Individual Ranking Signals', fontsize=14)\nax1.set_xticks(x)\nax1.set_xticklabels(doc_names)\nax1.legend()\nax1.grid(alpha=0.3, axis='y')\n\n# Show hybrid scores across different alpha values\nfor i, doc_name in enumerate(doc_names):\n    hybrid_scores = [alpha * bm25_norm[i] + (1-alpha) * semantic_norm[i] \n                     for alpha in alphas]\n    ax2.plot(alphas, hybrid_scores, marker='o', label=doc_name, linewidth=2)\n\nax2.set_xlabel('α (weight on BM25)', fontsize=12)\nax2.set_ylabel('Hybrid Score', fontsize=12)\nax2.set_title('Hybrid Scores: Tuning Keyword vs Semantic Balance', fontsize=14)\nax2.legend()\nax2.grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nNotice how different documents rank differently depending on \\(\\alpha\\): - \\(\\alpha = 1\\): Pure keyword search (BM25 only) - \\(\\alpha = 0\\): Pure semantic search (embeddings only)\n- \\(\\alpha = 0.5\\): Balanced hybrid"
  },
  {
    "objectID": "posts/searching-in-vector-dbs/index.html#when-to-use-each-strategy",
    "href": "posts/searching-in-vector-dbs/index.html#when-to-use-each-strategy",
    "title": "Searching in vector DBs: BM25 and semantic search",
    "section": "When to Use Each Strategy",
    "text": "When to Use Each Strategy\nPure BM25 (α ≈ 1.0): - Product catalogs with exact SKUs/model numbers - Legal documents with specific terminology - Code search where exact function names matter - Medical records with precise diagnoses\nPure Semantic (α ≈ 0.0): - Conversational queries with paraphrasing - Multi-language search - Concept-based discovery - When you want “similar ideas” not “same words”\nHybrid (α ≈ 0.5): - General-purpose search - E-commerce (combine exact matches + similar products) - Customer support (find relevant tickets even with different wording) - Most real-world applications"
  },
  {
    "objectID": "posts/searching-in-vector-dbs/index.html#real-world-performance",
    "href": "posts/searching-in-vector-dbs/index.html#real-world-performance",
    "title": "Searching in vector DBs: BM25 and semantic search",
    "section": "Real-World Performance",
    "text": "Real-World Performance\nHere’s what hybrid search looks like in practice:\n\n\n\nQuery Type\nBM25 Only\nSemantic Only\nHybrid\n\n\n\n\nExact match (“GPT-4”)\n✓✓✓\n✗\n✓✓✓\n\n\nParaphrase (“AI chatbot”)\n✗\n✓✓✓\n✓✓✓\n\n\nRare terms\n✓✓✓\n✗\n✓✓✓\n\n\nTypos\n✗\n✓✓\n✓✓\n\n\nMulti-concept\n✓\n✓✓✓\n✓✓✓\n\n\n\nStudies show hybrid search typically improves retrieval metrics by 15-30% over either method alone."
  },
  {
    "objectID": "posts/searching-in-vector-dbs/index.html#implementation-in-popular-vector-dbs",
    "href": "posts/searching-in-vector-dbs/index.html#implementation-in-popular-vector-dbs",
    "title": "Searching in vector DBs: BM25 and semantic search",
    "section": "Implementation in Popular Vector DBs",
    "text": "Implementation in Popular Vector DBs\nMost vector databases now support hybrid search out of the box:\nWeaviate:\nresults = client.query.get(\"Document\", [\"text\"]).with_hybrid(\n    query=\"BM25 search\",\n    alpha=0.5  # 0=pure vector, 1=pure BM25\n).do()\nPinecone:\nresults = index.query(\n    vector=embedding,\n    sparse_vector=sparse_vector,  # BM25-style sparse vector\n    top_k=10\n)\nElasticsearch:\n{\n  \"query\": {\n    \"hybrid\": {\n      \"queries\": [\n        {\"match\": {\"text\": \"BM25 search\"}},\n        {\"knn\": {\"field\": \"embedding\", \"query_vector\": [...]}}\n      ]\n    }\n  }\n}"
  },
  {
    "objectID": "posts/searching-in-vector-dbs/index.html#the-bottom-line",
    "href": "posts/searching-in-vector-dbs/index.html#the-bottom-line",
    "title": "Searching in vector DBs: BM25 and semantic search",
    "section": "The Bottom Line",
    "text": "The Bottom Line\nBM25 isn’t obsolete - it’s complementary. While embeddings capture semantic meaning beautifully, BM25 excels at exact keyword matching and term frequency analysis. Real production systems use both.\nThe 40-year-old algorithm survives because it solves problems that pure neural approaches don’t: exact matches, rare terms, and interpretable scores. Combined with modern embeddings, you get the best of both worlds.\nNext time you’re building search, don’t choose between BM25 and vectors. Use both."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "This Week I Learned",
    "section": "",
    "text": "Welcome to my technical deep dives! Every week (ish (I hope)) I explore how something interesting actually works under the hood.\nNote: none of this is original research, it is just summary to help with my own understanding.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSearching in vector DBs: BM25 and semantic search\n\n\n\nsearch\n\ninformation-retrieval\n\nvector-databases\n\nnlp\n\nembeddings\n\n\n\n\n\n\n\n\n\nOct 27, 2025\n\n\nDerek Allums\n\n\n\n\n\n\n\n\n\n\n\n\nWhy can L₁ regularization perform feature selection?\n\n\n\nregularization\n\ndata-science\n\nmachine-learning\n\nlasso\n\nl1\n\nridge\n\nl2\n\n\n\n\n\n\n\n\n\nOct 11, 2025\n\n\nDerek Allums\n\n\n\n\n\n\n\n\n\n\n\n\nHow Temperature Actually Works in LLMs\n\n\n\ntemperature\n\nllms\n\nmachine-learning\n\nRAG\n\n\n\n\n\n\n\n\n\nOct 11, 2025\n\n\nDerek Allums\n\n\n\n\n\nNo matching items"
  }
]