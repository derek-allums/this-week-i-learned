[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Derek Allums, I have a PhD in math and have spent most of my career as a data scientist and recently a data engineer. I realized there’s a lot of topics I have some passing familiarity with, e.g., “\\(L_1\\) regularization sometimes acts as feature selection”, but for many of these, how it works under the hood isn’t as clear (in this case, it comes back to geometry - see my second post).\nThe goal of this blog is to provide an outlet for these explorations in the hopes that maybe it could be useful for others - but mainly for me, since I find there’s no better way to understand something than to write about it since any gaps in the understanding become very clear."
  },
  {
    "objectID": "posts/temperature-llms/index.html",
    "href": "posts/temperature-llms/index.html",
    "title": "How Temperature Actually Works in LLMs",
    "section": "",
    "text": "Every time you use an LLM, there’s a parameter called “temperature” typically set at \\(0.7\\) or even hidden from the user, fundamentally shaping how creative or conservative the responses are.\nBut what’s actually happening under the hood?"
  },
  {
    "objectID": "posts/temperature-llms/index.html#introduction",
    "href": "posts/temperature-llms/index.html#introduction",
    "title": "How Temperature Actually Works in LLMs",
    "section": "",
    "text": "Every time you use an LLM, there’s a parameter called “temperature” typically set at \\(0.7\\) or even hidden from the user, fundamentally shaping how creative or conservative the responses are.\nBut what’s actually happening under the hood?"
  },
  {
    "objectID": "posts/temperature-llms/index.html#the-core-problem-probability-distributions",
    "href": "posts/temperature-llms/index.html#the-core-problem-probability-distributions",
    "title": "How Temperature Actually Works in LLMs",
    "section": "The Core Problem: Probability Distributions",
    "text": "The Core Problem: Probability Distributions\nAt its core, a language model outputs a probability distribution over its vocabulary for the next token.\nHere’s a toy exmaple on a simple vocabulary of 4 words:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Raw logits from the model\nlogits = np.array([2.0, 1.0, 0.5, 0.1])\nvocab = ['the', 'a', 'an', 'some']\n\n# Convert to probabilities with softmax\ndef softmax(x):\n    exp_x = np.exp(x - np.max(x))  # Subtract max for numerical stability\n    return exp_x / exp_x.sum()\n\nprobs = softmax(logits)\n\n# Visualize\nplt.figure(figsize=(10, 5))\nplt.bar(vocab, probs, color='steelblue')\nplt.ylabel('Probability')\nplt.title('Token Probabilities (Temperature = 1.0)')\nplt.ylim(0, 1)\nfor i, (word, prob) in enumerate(zip(vocab, probs)):\n    plt.text(i, prob + 0.02, f'{prob:.3f}', ha='center')\nplt.show()\n\n\n\n\n\n\n\n\nThe model might assign probabilities like: the: 0.52, a: 0.29, an: 0.13, some: 0.06"
  },
  {
    "objectID": "posts/temperature-llms/index.html#the-mathematics-of-temperature",
    "href": "posts/temperature-llms/index.html#the-mathematics-of-temperature",
    "title": "How Temperature Actually Works in LLMs",
    "section": "The Mathematics of Temperature",
    "text": "The Mathematics of Temperature\nTemperature \\(T\\) is applied before the softmax function. The modified softmax becomes:\n\\[\nP(x_i) = \\frac{e^{z_i/T}}{\\sum_{j=1}^{n} e^{z_j/T}}\n\\]\nWhere: - \\(z_i\\) are the raw logits from the model - \\(T\\) is the temperature parameter - \\(P(x_i)\\) is the probability of token \\(i\\)\nFor those famiilar with, e.g., OpenAI’s API, you’re immediately wondering “wait I thought it was between 0 and 2? This could be between \\(\\epsilon &gt; 0\\) and \\(\\infty\\).” True - they basically normalize these values.\n\nWhat Different Temperatures Do\nOne way to think about this is it’s basically a dampening effect. When we say “creativity” we mean (I guess) “randomness”. Conversely, “deterministic” is just that.\n\ndef softmax_with_temperature(logits, temperature):\n    scaled_logits = logits / temperature\n    return softmax(scaled_logits)\n\ntemperatures = [0.1, 0.5, 1.0, 2.0, 5.0]\n\nfig, axes = plt.subplots(2, 3, figsize=(15, 8))\naxes = axes.flatten()\n\nfor idx, temp in enumerate(temperatures):\n    probs_temp = softmax_with_temperature(logits, temp)\n    \n    axes[idx].bar(vocab, probs_temp, color='steelblue')\n    axes[idx].set_ylabel('Probability')\n    axes[idx].set_title(f'Temperature = {temp}')\n    axes[idx].set_ylim(0, 1)\n    \n    for i, (word, prob) in enumerate(zip(vocab, probs_temp)):\n        axes[idx].text(i, prob + 0.02, f'{prob:.3f}', ha='center', fontsize=9)\n\n# Remove extra subplot\nfig.delaxes(axes[5])\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nThe Entropy Perspective\nContinuing the comments above, we can sort of formalize this with entropy, thinking of it as a measure of the “randomness” of a distribution:\n\\[\nH(P) = -\\sum_{i=1}^{n} P(x_i) \\log P(x_i)\n\\]\nHigher temperature → higher entropy → more random outputs\n\ndef entropy(probs):\n    return -np.sum(probs * np.log(probs + 1e-10))\n\ntemp_range = np.linspace(0.1, 5.0, 100)\nentropies = [entropy(softmax_with_temperature(logits, t)) for t in temp_range]\n\nplt.figure(figsize=(10, 5))\nplt.plot(temp_range, entropies, linewidth=2, color='darkred')\nplt.xlabel('Temperature')\nplt.ylabel('Entropy (bits)')\nplt.title('How Temperature Affects Distribution Entropy')\nplt.grid(alpha=0.3)\nplt.show()"
  },
  {
    "objectID": "posts/temperature-llms/index.html#real-implementation",
    "href": "posts/temperature-llms/index.html#real-implementation",
    "title": "How Temperature Actually Works in LLMs",
    "section": "Real Implementation",
    "text": "Real Implementation\nHere’s how this looks in actual PyTorch code:\nimport torch\nimport torch.nn.functional as F\n\ndef sample_with_temperature(logits, temperature=1.0):\n    \"\"\"\n    Sample from a language model with temperature scaling.\n    \n    Args:\n        logits: Raw output scores from model (shape: [vocab_size])\n        temperature: Sampling temperature (default: 1.0)\n    \n    Returns:\n        Sampled token index\n    \"\"\"\n    # Scale logits by temperature\n    scaled_logits = logits / temperature\n    \n    # Convert to probabilities\n    probs = F.softmax(scaled_logits, dim=-1)\n    \n    # Sample from the distribution\n    token_id = torch.multinomial(probs, num_samples=1)\n    \n    return token_id\n\n# Example usage\nlogits = torch.tensor([2.0, 1.0, 0.5, 0.1])\n\n# Sample 1000 times at different temperatures\nfor temp in [0.5, 1.0, 2.0]:\n    samples = [sample_with_temperature(logits, temp).item() \n               for _ in range(1000)]\n    \n    # Count occurrences\n    counts = np.bincount(samples, minlength=4)\n    empirical_probs = counts / 1000\n    \n    print(f\"\\nTemperature {temp}:\")\n    for word, prob in zip(vocab, empirical_probs):\n        print(f\"  {word}: {prob:.3f}\")"
  },
  {
    "objectID": "posts/temperature-llms/index.html#edge-cases",
    "href": "posts/temperature-llms/index.html#edge-cases",
    "title": "How Temperature Actually Works in LLMs",
    "section": "Edge Cases",
    "text": "Edge Cases\nTemperature = 0: Not technically possible (division by zero), but temperature → 0 approximates greedy decoding (always pick the highest probability token). Thinking back to teh definition and exmaples above, you basically exaggerate the differences in probabilities between the next tokens, so that eventually it’s deterministic or very close to it.\nTemperature &lt; 1: “Sharpens” the distribution, making high-probability tokens even more likely.\nTemperature = 1: No modification, the original model distribution.\nTemperature &gt; 1: “Flattens” the distribution, giving lower-probability tokens more chance."
  },
  {
    "objectID": "posts/temperature-llms/index.html#practical-guidelines",
    "href": "posts/temperature-llms/index.html#practical-guidelines",
    "title": "How Temperature Actually Works in LLMs",
    "section": "Practical Guidelines",
    "text": "Practical Guidelines\nHere are some guidelines that you can find all over the internet (referencing the OpenAI normalized values). Better heuristics are available as well.\n\nT = 0.1-0.3: Factual tasks, code generation, when you need consistency\nT = 0.7-0.9: General conversation, slight creativity\nT = 1.0-1.5: Creative writing, brainstorming\nT = 2.0+: Experimental/chaotic, rarely useful"
  },
  {
    "objectID": "posts/l1-versus-l2/index.html",
    "href": "posts/l1-versus-l2/index.html",
    "title": "Why can L₁ regularization perform feature selection?",
    "section": "",
    "text": "If you’ve ever trained a regression model, you’ve probably heard: “Use L₁ for feature selection, L₂ for handling multicollinearity.” But why does L₁ regularization zero out coefficients while L₂ just makes them smaller? The answer lies in geometry.\nToday we’re diving into the geometric intuition behind why L₁ (Lasso) can eliminate features entirely, while L₂ (Ridge) cannot."
  },
  {
    "objectID": "posts/l1-versus-l2/index.html#introduction",
    "href": "posts/l1-versus-l2/index.html#introduction",
    "title": "Why can L₁ regularization perform feature selection?",
    "section": "",
    "text": "If you’ve ever trained a regression model, you’ve probably heard: “Use L₁ for feature selection, L₂ for handling multicollinearity.” But why does L₁ regularization zero out coefficients while L₂ just makes them smaller? The answer lies in geometry.\nToday we’re diving into the geometric intuition behind why L₁ (Lasso) can eliminate features entirely, while L₂ (Ridge) cannot."
  },
  {
    "objectID": "posts/l1-versus-l2/index.html#the-setup-what-are-we-optimizing",
    "href": "posts/l1-versus-l2/index.html#the-setup-what-are-we-optimizing",
    "title": "Why can L₁ regularization perform feature selection?",
    "section": "The Setup: What Are We Optimizing?",
    "text": "The Setup: What Are We Optimizing?\nIn regularized regression, we’re trying to minimize a loss function plus a penalty term:\n\\[\n\\text{minimize: } \\mathcal{L}(\\mathbf{w}) + \\lambda \\cdot \\text{Penalty}(\\mathbf{w})\n\\]\nWhere: - \\(\\mathcal{L}(\\mathbf{w})\\) is our loss function (e.g., mean squared error) - \\(\\lambda\\) controls the regularization strength - The penalty differs between L₁ and L₂\nL₁ (Lasso) penalty: \\[\n\\text{Penalty}_{\\text{L1}} = \\sum_{i=1}^{n} |w_i|\n\\]\nL₂ (Ridge) penalty: \\[\n\\text{Penalty}_{\\text{L2}} = \\sum_{i=1}^{n} w_i^2\n\\]\nThe key question: why does one drive coefficients to exactly zero while the other doesn’t?"
  },
  {
    "objectID": "posts/l1-versus-l2/index.html#the-geometric-intuition",
    "href": "posts/l1-versus-l2/index.html#the-geometric-intuition",
    "title": "Why can L₁ regularization perform feature selection?",
    "section": "The Geometric Intuition",
    "text": "The Geometric Intuition\nLet’s visualize this in 2D (two coefficients: \\(w_1\\) and \\(w_2\\)). We can reframe the optimization as a constrained problem:\nMinimize the loss subject to: - L₁: \\(|w_1| + |w_2| \\leq t\\) - L₂: \\(w_1^2 + w_2^2 \\leq t\\)\nHere’s what these constraint regions look like:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Create figure with two subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n\n# L1 constraint region (diamond)\nt = 1.0\nw1_l1 = np.array([t, 0, -t, 0, t])\nw2_l1 = np.array([0, t, 0, -t, 0])\n\nax1.fill(w1_l1, w2_l1, alpha=0.3, color='blue', label='Feasible region')\nax1.plot(w1_l1, w2_l1, 'b-', linewidth=2)\nax1.axhline(y=0, color='k', linestyle='-', alpha=0.3)\nax1.axvline(x=0, color='k', linestyle='-', alpha=0.3)\nax1.set_xlabel('w₁', fontsize=12)\nax1.set_ylabel('w₂', fontsize=12)\nax1.set_title('L₁ Constraint: |w₁| + |w₂| ≤ 1', fontsize=14)\nax1.set_xlim(-1.5, 1.5)\nax1.set_ylim(-1.5, 1.5)\nax1.grid(alpha=0.3)\nax1.legend()\n\n# L2 constraint region (circle)\ntheta = np.linspace(0, 2*np.pi, 100)\nw1_l2 = t * np.cos(theta)\nw2_l2 = t * np.sin(theta)\n\nax2.fill(w1_l2, w2_l2, alpha=0.3, color='red', label='Feasible region')\nax2.plot(w1_l2, w2_l2, 'r-', linewidth=2)\nax2.axhline(y=0, color='k', linestyle='-', alpha=0.3)\nax2.axvline(x=0, color='k', linestyle='-', alpha=0.3)\nax2.set_xlabel('w₁', fontsize=12)\nax2.set_ylabel('w₂', fontsize=12)\nax2.set_title('L₂ Constraint: w₁² + w₂² ≤ 1', fontsize=14)\nax2.set_xlim(-1.5, 1.5)\nax2.set_ylim(-1.5, 1.5)\nax2.grid(alpha=0.3)\nax2.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nNotice the key difference: - L₁ constraint = Diamond shape with sharp corners at the axes - L₂ constraint = Circle with no corners"
  },
  {
    "objectID": "posts/l1-versus-l2/index.html#where-solutions-live",
    "href": "posts/l1-versus-l2/index.html#where-solutions-live",
    "title": "Why can L₁ regularization perform feature selection?",
    "section": "Where Solutions Live",
    "text": "Where Solutions Live\nThe optimal solution occurs where the loss function contours (ellipses centered at the unconstrained minimum) first touch the constraint region.\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n\n# Loss function contours (ellipses centered away from origin)\ncenter_x, center_y = 0.8, 0.6\n\n# Create elliptical contours\ntheta = np.linspace(0, 2*np.pi, 100)\nfor scale in [0.3, 0.6, 0.9]:\n    ellipse_x = center_x + scale * 0.8 * np.cos(theta)\n    ellipse_y = center_y + scale * 0.6 * np.sin(theta)\n    ax1.plot(ellipse_x, ellipse_y, 'g--', alpha=0.6, linewidth=1)\n    ax2.plot(ellipse_x, ellipse_y, 'g--', alpha=0.6, linewidth=1)\n\n# L1 constraint\nax1.fill(w1_l1, w2_l1, alpha=0.2, color='blue')\nax1.plot(w1_l1, w2_l1, 'b-', linewidth=2, label='L₁ constraint')\nax1.plot(1, 0, 'ro', markersize=12, label='Solution (sparse!)')\nax1.axhline(y=0, color='k', linestyle='-', alpha=0.3)\nax1.axvline(x=0, color='k', linestyle='-', alpha=0.3)\nax1.set_xlabel('w₁', fontsize=12)\nax1.set_ylabel('w₂', fontsize=12)\nax1.set_title('L₁: Solution hits corner → w₂ = 0', fontsize=14)\nax1.set_xlim(-0.2, 1.5)\nax1.set_ylim(-0.2, 1.2)\nax1.grid(alpha=0.3)\nax1.legend()\n\n# L2 constraint\nax2.fill(w1_l2, w2_l2, alpha=0.2, color='red')\nax2.plot(w1_l2, w2_l2, 'r-', linewidth=2, label='L₂ constraint')\nax2.plot(0.65, 0.48, 'ro', markersize=12, label='Solution (both non-zero)')\nax2.axhline(y=0, color='k', linestyle='-', alpha=0.3)\nax2.axvline(x=0, color='k', linestyle='-', alpha=0.3)\nax2.set_xlabel('w₁', fontsize=12)\nax2.set_ylabel('w₂', fontsize=12)\nax2.set_title('L₂: Solution on smooth curve → all w ≠ 0', fontsize=14)\nax2.set_xlim(-0.2, 1.5)\nax2.set_ylim(-0.2, 1.2)\nax2.grid(alpha=0.3)\nax2.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe key insight:\n\nL₁: The diamond has corners that lie exactly on the axes. When the loss contours expand outward and touch the constraint region, they’re very likely to hit a corner first — where one or more coefficients are exactly zero.\nL₂: The circle has no corners. The loss contours will touch it at some smooth point where all coefficients are small but non-zero."
  },
  {
    "objectID": "posts/l1-versus-l2/index.html#why-corners-matter",
    "href": "posts/l1-versus-l2/index.html#why-corners-matter",
    "title": "Why can L₁ regularization perform feature selection?",
    "section": "Why Corners Matter",
    "text": "Why Corners Matter\nThink of it this way: in 2D, the L₁ diamond has 4 corners (one on each axis). As you increase dimensions, the number of corners grows exponentially. In \\(n\\) dimensions, there are \\(2n\\) corners, each representing a solution where \\((n-1)\\) coefficients are zero.\nThe L₂ sphere has no corners in any dimension — it’s perfectly smooth everywhere.\n\n# Probability of hitting a corner vs dimension\ndimensions = np.arange(2, 21)\n\n# Rough approximation: corners vs total surface area\n# For L1: 2n corners in n dimensions\n# For L2: 0 corners\n\n# Calculate relative \"sparsity inducing\" potential\nl1_sparsity = 2 * dimensions  # Number of fully sparse corners\ntotal_volume = dimensions ** 2  # Rough surface complexity\n\nsparsity_probability = l1_sparsity / total_volume\n\nplt.figure(figsize=(10, 6))\nplt.plot(dimensions, sparsity_probability, 'b-', linewidth=2, marker='o')\nplt.xlabel('Number of Dimensions', fontsize=12)\nplt.ylabel('Relative Likelihood of Hitting Axis\\n(Sparse Solution)', fontsize=12)\nplt.title('Why L₁ Induces Sparsity: More Corners in Higher Dimensions', fontsize=14)\nplt.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/l1-versus-l2/index.html#the-gradient-perspective",
    "href": "posts/l1-versus-l2/index.html#the-gradient-perspective",
    "title": "Why can L₁ regularization perform feature selection?",
    "section": "The Gradient Perspective",
    "text": "The Gradient Perspective\nThere’s another way to understand this: look at the gradients.\nL₁ gradient: \\[\n\\frac{\\partial}{\\partial w_i} |w_i| = \\begin{cases}\n+1 & \\text{if } w_i &gt; 0 \\\\\n-1 & \\text{if } w_i &lt; 0 \\\\\n\\text{undefined} & \\text{if } w_i = 0\n\\end{cases}\n\\]\nThe gradient is constant regardless of how small \\(w_i\\) gets! This creates a “constant push” toward zero.\nL₂ gradient: \\[\n\\frac{\\partial}{\\partial w_i} w_i^2 = 2w_i\n\\]\nAs \\(w_i\\) approaches zero, the gradient also approaches zero. The “push” weakens, so coefficients get very small but never quite reach zero.\n\n# Visualize the penalty and gradient behavior\nw = np.linspace(-2, 2, 1000)\nl1_penalty = np.abs(w)\nl2_penalty = w**2\n\n# Gradients (subgradient for L1 at 0)\nl1_grad = np.sign(w)\nl1_grad[w == 0] = 0  # Convention for visualization\nl2_grad = 2 * w\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n# Penalty functions\nax1.plot(w, l1_penalty, 'b-', linewidth=2, label='L₁: |w|')\nax1.plot(w, l2_penalty, 'r-', linewidth=2, label='L₂: w²')\nax1.axhline(y=0, color='k', linestyle='-', alpha=0.3)\nax1.axvline(x=0, color='k', linestyle='--', alpha=0.3)\nax1.set_xlabel('w', fontsize=12)\nax1.set_ylabel('Penalty', fontsize=12)\nax1.set_title('Penalty Functions', fontsize=14)\nax1.legend(fontsize=11)\nax1.grid(alpha=0.3)\nax1.set_ylim(-0.1, 2)\n\n# Gradients\nax2.plot(w, l1_grad, 'b-', linewidth=2, label='∂|w|/∂w (constant)')\nax2.plot(w, l2_grad, 'r-', linewidth=2, label='∂w²/∂w = 2w (linear)')\nax2.axhline(y=0, color='k', linestyle='-', alpha=0.3)\nax2.axvline(x=0, color='k', linestyle='--', alpha=0.3)\nax2.set_xlabel('w', fontsize=12)\nax2.set_ylabel('Gradient', fontsize=12)\nax2.set_title('Gradient Behavior', fontsize=14)\nax2.legend(fontsize=11)\nax2.grid(alpha=0.3)\nax2.set_ylim(-2.5, 2.5)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nNotice: L₁’s gradient stays at ±1 no matter how small the weight is, maintaining constant pressure to reach exactly zero. L₂’s gradient weakens as weights shrink, never quite finishing the job."
  },
  {
    "objectID": "posts/l1-versus-l2/index.html#real-example-sparse-vs-dense-solutions",
    "href": "posts/l1-versus-l2/index.html#real-example-sparse-vs-dense-solutions",
    "title": "Why can L₁ regularization perform feature selection?",
    "section": "Real Example: Sparse vs Dense Solutions",
    "text": "Real Example: Sparse vs Dense Solutions\nLet’s see this in action with actual regression:\n\nfrom sklearn.linear_model import Lasso, Ridge\nfrom sklearn.datasets import make_regression\nfrom sklearn.preprocessing import StandardScaler\n\n# Create a dataset with 100 features, only 10 truly relevant\nX, y = make_regression(n_samples=200, n_features=100, n_informative=10, \n                       noise=10, random_state=42)\n\n# Standardize\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Fit L1 (Lasso) and L2 (Ridge)\nlasso = Lasso(alpha=0.1, random_state=42)\nridge = Ridge(alpha=0.1, random_state=42)\n\nlasso.fit(X_scaled, y)\nridge.fit(X_scaled, y)\n\n# Count non-zero coefficients\nlasso_nonzero = np.sum(np.abs(lasso.coef_) &gt; 1e-5)\nridge_nonzero = np.sum(np.abs(ridge.coef_) &gt; 1e-5)\n\nprint(f\"L₁ (Lasso): {lasso_nonzero} non-zero coefficients out of 100\")\nprint(f\"L₂ (Ridge): {ridge_nonzero} non-zero coefficients out of 100\")\n\n# Visualize coefficient magnitudes\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n# Lasso coefficients\nax1.bar(range(100), np.abs(lasso.coef_), color='blue', alpha=0.7)\nax1.set_xlabel('Feature Index', fontsize=12)\nax1.set_ylabel('|Coefficient|', fontsize=12)\nax1.set_title(f'L₁ (Lasso): {lasso_nonzero} features selected', fontsize=14)\nax1.grid(alpha=0.3, axis='y')\n\n# Ridge coefficients\nax2.bar(range(100), np.abs(ridge.coef_), color='red', alpha=0.7)\nax2.set_xlabel('Feature Index', fontsize=12)\nax2.set_ylabel('|Coefficient|', fontsize=12)\nax2.set_title(f'L₂ (Ridge): All {ridge_nonzero} features retained', fontsize=14)\nax2.grid(alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.show()\n\nL₁ (Lasso): 81 non-zero coefficients out of 100\nL₂ (Ridge): 100 non-zero coefficients out of 100\n\n\n\n\n\n\n\n\n\nLasso aggressively zeros out irrelevant features, while Ridge shrinks everything but keeps all features in play."
  },
  {
    "objectID": "posts/l1-versus-l2/index.html#when-to-use-each",
    "href": "posts/l1-versus-l2/index.html#when-to-use-each",
    "title": "Why can L₁ regularization perform feature selection?",
    "section": "When to Use Each",
    "text": "When to Use Each\nUse L₁ (Lasso) when: - You have many features and suspect most are irrelevant - You want automatic feature selection - Model interpretability matters (fewer features = easier to explain) - You’re okay with potentially discarding correlated features arbitrarily\nUse L₂ (Ridge) when: - You have multicollinearity and want to keep all features - Features are all potentially relevant - You don’t need exact zeros in your coefficient vector - You want more stable coefficient estimates\nUse Elastic Net when: - You want the best of both worlds (combines L₁ and L₂) - You have groups of correlated features you want to select together"
  },
  {
    "objectID": "posts/l1-versus-l2/index.html#the-bottom-line",
    "href": "posts/l1-versus-l2/index.html#the-bottom-line",
    "title": "Why can L₁ regularization perform feature selection?",
    "section": "The Bottom Line",
    "text": "The Bottom Line\nL₁ regularization performs feature selection because of geometry: its constraint region has sharp corners on the coordinate axes. When the optimization finds the best solution within this diamond-shaped region, it naturally tends to land on these corners — where coefficients are exactly zero.\nL₂’s smooth circular constraint has no such corners, so solutions have small but non-zero coefficients across all features.\nIt’s a beautiful example of how the shape of a constraint can fundamentally change the nature of solutions you get.\n\nNext week: How Go channels actually work under the hood (spoiler: circular buffers and mutexes)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "This Week I Learned",
    "section": "",
    "text": "Welcome to my technical deep dives! Every week I explore how something interesting actually works under the hood.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy can L₁ regularization perform feature selection?\n\n\n\nregularization\n\ndata-science\n\nmachine-learning\n\nlasso\n\nl1\n\nridge\n\nl2\n\n\n\n\n\n\n\n\n\nOct 11, 2025\n\n\nDerek Allums\n\n\n\n\n\n\n\n\n\n\n\n\nHow Temperature Actually Works in LLMs\n\n\n\ntemperature\n\nllms\n\nmachine-learning\n\nRAG\n\n\n\n\n\n\n\n\n\nOct 11, 2025\n\n\nDerek Allums\n\n\n\n\n\nNo matching items"
  }
]